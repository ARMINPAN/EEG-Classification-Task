{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vGvaVdOgfdAt"
      },
      "outputs": [],
      "source": [
        "# EEG Classification BCI Task\n",
        "# Computational Intelligence Course Final Project\n",
        "# Armin Panjehpour - 98101288"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzJtcbH1cBbd",
        "outputId": "b3b8a722-5022-4d02-dabe-3d99f34803cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Accessing google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/CI_Project_Dataset/'  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "id": "kiGddLPUcPHC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "from scipy.io import loadmat\n",
        "from scipy.fft import fft\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from scipy import signal\n",
        "from sklearn.model_selection import KFold\n",
        "import scipy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-gTSxuddtGM",
        "outputId": "ecb11509-a4ae-4dbb-800b-061e9de1ec3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'TestData', 'TrainData', 'TrainLabel'])\n",
            "Dataset loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load Dataset\n",
        "Data = mat = loadmat('/content/gdrive/MyDrive/Colab Notebooks/CI_Project_Dataset/CI_Project_data.mat')\n",
        "print(Data.keys())\n",
        "print('Dataset loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lnmmcb9gjNG",
        "outputId": "8d47fffb-18bf-4e7f-d671-15a50fdf5315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_Data:  (30, 384, 120)\n",
            "Test_Data:  (30, 384, 40)\n",
            "Train_Label:  (120,)\n",
            "Train\\Test Data Created!\n"
          ]
        }
      ],
      "source": [
        "# Training and Test Dataset\n",
        "\n",
        "Train_Data = Data['TrainData']\n",
        "Test_Data = Data['TestData']\n",
        "\n",
        "# Training Dataset Labels\n",
        "Labels = np.squeeze(Data['TrainLabel'])-1\n",
        "\n",
        "# Dataset Size\n",
        "print('Train_Data: ',Train_Data.shape)\n",
        "print('Test_Data: ',Test_Data.shape)\n",
        "print('Train_Label: ',Labels.shape)\n",
        "\n",
        "\n",
        "print('Train\\Test Data Created!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XDi6kclhKq-",
        "outputId": "fb1f1ca1-1dff-4f2e-a833-17078f8b6245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling Rate is:  256.0\n"
          ]
        }
      ],
      "source": [
        "# Sampling Rate\n",
        "\n",
        "Trial_Time = 1.5 # in seconds\n",
        "Fs = Train_Data.shape[1]/Trial_Time\n",
        "\n",
        "print('Sampling Rate is: ',Fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "83f3lsRYfPwy"
      },
      "outputs": [],
      "source": [
        "# different frequency bands in Hz\n",
        "Delta_freq = [0.5, 3] \n",
        "Theta_freq = [4, 7]\n",
        "Alpha_freq = [8, 12]\n",
        "betha_freq = [12, 30]\n",
        "Gamma_freq = [30, 100]\n",
        "\n",
        "selected_frequency_band = betha_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaX7yUV5kEt2",
        "outputId": "a06b40ef-3ffe-4270-9a53-f69202306067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 384, 100) (30, 384, 20) (100,) (20,)\n"
          ]
        }
      ],
      "source": [
        "# Train-Validation Split\n",
        "\n",
        "# select 20% of train data as validation data\n",
        "n_val_class1 = 10\n",
        "n_val_class2 = 10\n",
        "selected_class1 = np.random.choice(np.squeeze(np.where(Labels == 0)), size=n_val_class1, replace=False)\n",
        "selected_class2 = np.random.choice(np.squeeze(np.where(Labels == 1)), size=n_val_class2, replace=False)\n",
        "selected_valdata = np.concatenate((selected_class1, selected_class2))\n",
        "\n",
        "\n",
        "Val_data = Train_Data[:,:,selected_valdata]\n",
        "Val_Label = Labels[selected_valdata]\n",
        "Train_Data = np.delete(Train_Data, selected_valdata, axis=2)\n",
        "Train_Label = np.delete(Labels, selected_valdata)\n",
        "\n",
        "print(Train_Data.shape, Val_data.shape, Train_Label.shape, Val_Label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPGi1ZRdAoOW",
        "outputId": "027b09b5-336c-4eab-d1b7-c5414fe2ff9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data normalized!\n"
          ]
        }
      ],
      "source": [
        "# normalize data\n",
        "amp_norm = np.max(Train_Data, axis=(1,2))\n",
        "std_norm = np.std(Train_Data, axis=(1,2))\n",
        "\n",
        "for i in range(Train_Data.shape[0]):\n",
        "  Train_Data[i,:,:] = (Train_Data[i,:,:] - amp_norm[i])/std_norm[i]\n",
        "  Val_data[i,:,:] = (Val_data[i,:,:] - amp_norm[i])/std_norm[i]\n",
        "\n",
        "print('data normalized!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd4SptO-edEd",
        "outputId": "496ddad7-5fc0-494d-bf6b-c96c9ad3df3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 384, 100)\n"
          ]
        }
      ],
      "source": [
        "# Export different bands of data \n",
        "\n",
        "# design a bandpass filter\n",
        "order = 20\n",
        "sos = signal.butter(order, selected_frequency_band, btype='bandpass', fs=Fs, output='sos')\n",
        "\n",
        "# filter the data in time dimension\n",
        "Train_Data_filtered = signal.sosfilt(sos, Train_Data, axis=1)\n",
        "Test_Data_filtered = signal.sosfilt(sos, Test_Data, axis=1)\n",
        "Val_Data_filtered = signal.sosfilt(sos, Val_data, axis=1)\n",
        "\n",
        "\n",
        "# select unfiltered or filtered data\n",
        "selected_Data = Train_Data_filtered\n",
        "print(selected_Data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wgcPg7itlTn0"
      },
      "outputs": [],
      "source": [
        "############################################ Calculate Different Features for Each Channel ############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5L9BWbPDliHX"
      },
      "outputs": [],
      "source": [
        "def Var_Feature(data):\n",
        "  # variance of each channel\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "  var = np.var(data, axis=1)\n",
        "  return var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uGTQ_F1TmfzZ"
      },
      "outputs": [],
      "source": [
        "def amp_hist_Feature(data, n_bins, min_amp, max_amp):\n",
        "  # amplitude histogram\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "  n_channels = data.shape[0]\n",
        "  n_trials = data.shape[-1]\n",
        "  \n",
        "  hist_vals = np.zeros((n_channels,n_trials,n_bins))\n",
        "  for i in range(n_channels):\n",
        "    for j in range(n_trials):\n",
        "      in_range = np.asarray(np.where((np.logical_and(1 <= data[i,:,j], data[i,:,j] <= 2)) == True))\n",
        "      selected_chan_data = data[i, in_range, j]\n",
        "      hist_vals[i,j,:] = np.histogram(selected_chan_data, n_bins)[0]\n",
        "      \n",
        "  \n",
        "  return np.asarray(hist_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m8hhHtEvAB8n"
      },
      "outputs": [],
      "source": [
        "def AR_Coeffs(data, order):\n",
        "  # autoregressive model coefficients\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "  n_trials = data.shape[-1]\n",
        "  n_samples = data.shape[1]\n",
        "  n_channels = data.shape[0]\n",
        "  Y = np.zeros((n_channels, n_samples-order, n_trials))\n",
        "  Y = data[:, order:n_samples, :]\n",
        "\n",
        "  X = np.zeros((n_trials, n_channels, n_samples-order, order+1))\n",
        "  X[:,:,:,0] = 1\n",
        "\n",
        "  Coeffs = np.zeros((n_trials, n_channels, order+1))\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      for k in range(n_samples-order):\n",
        "        for z in range(order):\n",
        "          if(k-z >= 0):\n",
        "            X[i,j,k,order-z-1] = data[j,k-z,i]\n",
        "          else:\n",
        "            X[i,j,k,order-z-1] = 0\n",
        "\n",
        "      a = ((X[i,j,:,:].T @ X[i,j,:,:]) + np.asarray(0.00001*np.random.random((order+1, order+1))))\n",
        "      Coeffs[i,j,:] = np.linalg.inv(a) @ (X[i,j,:,:].T) @ Y[j,:,i]\n",
        "\n",
        "  return Coeffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aokkYWWggFhf"
      },
      "outputs": [],
      "source": [
        "def FF_Feature(data):\n",
        "  # form factor\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "  signal_std = np.std(data, axis=1)\n",
        "  first_deriv_Std = np.std(np.diff(data, axis=1), axis=1)\n",
        "  second_deriv_Std = np.std(np.diff(np.diff(data, axis=1), axis=1), axis=1)\n",
        "  FF = (second_deriv_Std/first_deriv_Std)/(first_deriv_Std/signal_std)\n",
        "\n",
        "  return FF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ev3PfmhmlJZE"
      },
      "outputs": [],
      "source": [
        "def cov_calculator(data1, data2):\n",
        "  # covariancce calculator between two vector signals\n",
        "  return np.sum(np.multiply(data1-np.mean(data1),data2-np.mean(data2)))/data1.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ctsYYd4QjaEq"
      },
      "outputs": [],
      "source": [
        "def cov_Feature(data):\n",
        "  # covariance between pairs of channels\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "\n",
        "  n_trials = data.shape[-1]\n",
        "  n_samples = data.shape[1]\n",
        "  n_channels = data.shape[0]\n",
        "\n",
        "  cov_matrix = np.zeros((n_trials,n_channels,n_channels))\n",
        "  cov_values = np.zeros((n_trials,int((n_channels-1)*n_channels/2)))\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      for k in range(j+1):\n",
        "        selected_data_ch1 = data[j,:,i]\n",
        "        selected_data_ch2 = data[k,:,i]\n",
        "        cov_values[i,j+k] = cov_calculator(selected_data_ch1,selected_data_ch2)\n",
        "\n",
        "  \n",
        "  return cov_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mQYtwf1ziVp9"
      },
      "outputs": [],
      "source": [
        "def kurtosis_Feature(data):\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "\n",
        "  n_trials = data.shape[-1]\n",
        "  n_samples = data.shape[1]\n",
        "  n_channels = data.shape[0]\n",
        "\n",
        "  kurtosis = np.zeros((n_trials,n_channels))\n",
        "\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      selected_data = data[j,:,i]\n",
        "      kurtosis[i,j] = stats.kurtosis(selected_data)\n",
        "\n",
        "\n",
        "  return kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uYD2IzhnopmV"
      },
      "outputs": [],
      "source": [
        "def fft_calculator(data, Fs):\n",
        "  # calculate single side band fft of a vector signal\n",
        "  L = data.shape[0]\n",
        "  fft_data = fft(data)\n",
        "  P2 = np.abs(fft_data/L)\n",
        "  P1 = P2[0:int(L/2)+1]\n",
        "  P1[1:-1] = 2*P1[1:-1]\n",
        "\n",
        "  f = Fs*np.arange(0,L/2+1)/L\n",
        "\n",
        "  return f, P1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UBZvqLIenhD4"
      },
      "outputs": [],
      "source": [
        "def max_freq_Feature(data, Fs):\n",
        "  # find the frequency with the maximum amplitude for each channel\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "  \n",
        "  n_trials = data.shape[-1]\n",
        "  n_samples = data.shape[1]\n",
        "  n_channels = data.shape[0]\n",
        "\n",
        "  max_freq = np.zeros((n_trials,n_channels))\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      selected_data = data[j,:,i]\n",
        "      f, fft_selected_data = fft_calculator(selected_data, Fs)\n",
        "      max_freq[i,j] = f[np.argmax(fft_selected_data)]\n",
        "\n",
        "  return f, max_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wTsngFbqu4MV"
      },
      "outputs": [],
      "source": [
        "def mean_freq_Feature(data, Fs):\n",
        "  # find the normalized weighted mean of frequencies\n",
        "  # dim1: channel number, dim2: time, dim3: trial number\n",
        "\n",
        "  n_trials = data.shape[-1]\n",
        "  n_samples = data.shape[1]\n",
        "  n_channels = data.shape[0]\n",
        "\n",
        "  mean_freq = np.zeros((n_trials,n_channels))\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      selected_data = data[j,:,i]\n",
        "      f, fft_selected_data = fft_calculator(selected_data, Fs)\n",
        "\n",
        "      mean_freq[i,j] = np.sum(np.multiply(f, fft_selected_data))/np.sum(fft_selected_data)\n",
        "\n",
        "\n",
        "  return f, mean_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "id": "wJkYYuxpC4A2"
      },
      "outputs": [],
      "source": [
        "# each band energy\n",
        "def band_energy_Feature(raw_data, Fs):\n",
        "  n_trials = raw_data.shape[-1]\n",
        "  n_samples = raw_data.shape[1]\n",
        "  n_channels = raw_data.shape[0]\n",
        "  n_bands = 5\n",
        "\n",
        "  band_energy = np.zeros((n_trials,n_channels,n_bands))\n",
        "  freqs = np.array([[0.5, 3], [4, 7], [8, 12], [12, 30], [30, 100]])\n",
        "\n",
        "\n",
        "  # energy of all bands\n",
        "\n",
        "  denum = np.zeros((n_trials,n_channels))\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      # sum over all energy bands\n",
        "      for k in range(n_bands):\n",
        "        # design a bandpass filter\n",
        "        selected_frequency_band = freqs[k,:]\n",
        "        order = 20\n",
        "        sos = signal.butter(order, selected_frequency_band, 'bandpass', fs=Fs, output='sos')\n",
        "\n",
        "        # filter the data in time dimension\n",
        "        Data_filtered = signal.sosfilt(sos, raw_data[j,:,i])\n",
        "\n",
        "        f, fft_selected_data = fft_calculator(Data_filtered, Fs)\n",
        "\n",
        "        denum[i,j] += np.sum(np.power(fft_selected_data, 2))\n",
        "\n",
        "    \n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      selected_data = raw_data[j,:,i]\n",
        "      for kk in range(n_bands):\n",
        "        # design a bandpass filter\n",
        "        selected_frequency_band = freqs[kk,:]\n",
        "        order = 20\n",
        "        sos = signal.butter(order, selected_frequency_band, 'bandpass', fs=Fs, output='sos')\n",
        "\n",
        "        # filter the data in time dimension\n",
        "        Data_filtered = signal.sosfilt(sos, selected_data)\n",
        "\n",
        "        f, fft_selected_data = fft_calculator(Data_filtered, Fs)\n",
        "\n",
        "        # energy of the selected frequency band\n",
        "        num = np.sum(np.power(fft_selected_data, 2))\n",
        "\n",
        "        print(i,j,k)\n",
        "        band_energy[i,j,kk] = num/denum[i,j]\n",
        "\n",
        "\n",
        "  return band_energy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each band energy\n",
        "def band_energy_Feature1(x, fs):\n",
        "\n",
        "  freqs = np.array([[0.5, 3], [4, 7], [8, 12], [12, 30], [30, 100]])\n",
        "\n",
        "  n_trials = x.shape[-1]\n",
        "  n_samples = x.shape[1]\n",
        "  n_channels = x.shape[0]\n",
        "  n_bands = 5\n",
        "  psd = np.zeros((n_trials,n_channels,n_bands))\n",
        "\n",
        "  for i in range(n_trials):\n",
        "    for j in range(n_channels):\n",
        "      selected_data = x[j,:,i]\n",
        "      for k in range(n_bands):\n",
        "        fmin = freqs[k,0]\n",
        "        fmax = freqs[k,1]\n",
        "        f, Pxx = scipy.signal.periodogram(selected_data, fs=fs)\n",
        "        ind_min = scipy.argmax(f > fmin) - 1\n",
        "        ind_max = scipy.argmax(f > fmax) - 1\n",
        "        psd[i,j,k] = scipy.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])\n",
        "        \n",
        "  return psd"
      ],
      "metadata": {
        "id": "YU-AIbpBjxBQ"
      },
      "execution_count": 449,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "vtQ2pN7gzbjC",
        "outputId": "112acad4-465c-471b-e059-13e66c3407c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcc652fa910>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "############################################ Averaged FFT over all trials and channels ############################################\n",
        "f, fffft = fft_calculator(Train_Data_filtered[1,:,1], Fs)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
        "ax.plot(f, fffft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "w98tO9jiwwj4"
      },
      "outputs": [],
      "source": [
        "############################################ Apply the Feature Functions on Training Data ############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyIrVWl_K0fi",
        "outputId": "fb1c2806-428f-4f81-93ab-c2d8a31ddee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50,) (50,) (10,) (10,)\n"
          ]
        }
      ],
      "source": [
        "# get trial numbers of two classes\n",
        "class1_trialnum = np.squeeze(np.asarray(np.where(np.squeeze(Train_Label) == 0)))\n",
        "class2_trialnum = np.squeeze(np.asarray(np.where(np.squeeze(Train_Label) == 1)))\n",
        "\n",
        "class1_trialnum_val = np.squeeze(np.asarray(np.where(np.squeeze(Val_Label) == 0)))\n",
        "class2_trialnum_val = np.squeeze(np.asarray(np.where(np.squeeze(Val_Label) == 1)))\n",
        "\n",
        "print(class1_trialnum.shape, class2_trialnum.shape, class1_trialnum_val.shape, class2_trialnum_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzOTtCamzWIn",
        "outputId": "62ab170c-f97a-42da-b0c7-422168bf524f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30)\n",
            "(2, 10, 30)\n"
          ]
        }
      ],
      "source": [
        "# calculate variance of each trail and channel - one value for each channel and trial for train data\n",
        "var_feature = Var_Feature(selected_Data)\n",
        "\n",
        "n_class = 2\n",
        "var_feature_classes = np.zeros((n_class,int(var_feature.shape[1]/n_class),var_feature.shape[0]))\n",
        "\n",
        "var_feature_classes[0,:,:] = var_feature.T[class2_trialnum,:]\n",
        "var_feature_classes[1,:,:] = var_feature.T[class2_trialnum,:]\n",
        "\n",
        "\n",
        "print(var_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "var_feature_val = Var_Feature(Val_Data_filtered)\n",
        "\n",
        "n_class = 2\n",
        "var_feature_classes_val = np.zeros((n_class,int(var_feature_val.shape[1]/n_class),var_feature_val.shape[0]))\n",
        "\n",
        "var_feature_classes_val[0,:,:] = var_feature_val.T[class1_trialnum_val,:]\n",
        "var_feature_classes_val[1,:,:] = var_feature_val.T[class2_trialnum_val,:]\n",
        "\n",
        "\n",
        "\n",
        "print(var_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9fiycA4z57y",
        "outputId": "e0120f6b-fec0-475d-f8aa-faccc825e267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 30, 50, 5)\n",
            "(2, 30, 10, 5)\n"
          ]
        }
      ],
      "source": [
        "# calculate amplitude histogram - nbin values for each channel for training data\n",
        "n_bins = 5\n",
        "min_amp = -10\n",
        "max_amp = 10\n",
        "hist_feature = amp_hist_Feature(selected_Data, n_bins, min_amp, max_amp)\n",
        "\n",
        "n_class = 2\n",
        "hist_feature_classes = np.zeros((n_class,hist_feature.shape[0],int(hist_feature.shape[1]/n_class),hist_feature.shape[-1]))\n",
        "\n",
        "hist_feature_classes[0,:,:,:] = hist_feature[:,class1_trialnum,:]\n",
        "hist_feature_classes[1,:,:,:] = hist_feature[:,class2_trialnum,:]\n",
        "\n",
        "print(hist_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "n_bins = 5\n",
        "min_amp = -10\n",
        "max_amp = 10\n",
        "hist_feature_val = amp_hist_Feature(Val_Data_filtered, n_bins, min_amp, max_amp)\n",
        "\n",
        "n_class = 2\n",
        "hist_feature_classes_val = np.zeros((n_class,hist_feature_val.shape[0],int(hist_feature_val.shape[1]/n_class),hist_feature_val.shape[-1]))\n",
        "\n",
        "hist_feature_classes_val[0,:,:,:] = hist_feature_val[:,class1_trialnum_val,:]\n",
        "hist_feature_classes_val[0,:,:,:] = hist_feature_val[:,class2_trialnum_val,:]\n",
        "\n",
        "print(hist_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tYHuTLH-DHL",
        "outputId": "e9e256ba-c8d7-402c-f2cf-98d5f1864d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30, 6)\n",
            "(2, 10, 30, 6)\n"
          ]
        }
      ],
      "source": [
        "# calculate AR coefficients - order values for each trail and channel\n",
        "order = 5\n",
        "ar_feature = AR_Coeffs(selected_Data, order)\n",
        "\n",
        "n_class = 2\n",
        "ar_feature_classes = np.zeros((n_class,int(ar_feature.shape[0]/n_class),ar_feature.shape[1],ar_feature.shape[-1]))\n",
        "\n",
        "ar_feature_classes[0,:,:,:] = ar_feature[class1_trialnum,:,:]\n",
        "ar_feature_classes[1,:,:,:] = ar_feature[class2_trialnum,:,:]\n",
        "\n",
        "print(ar_feature_classes.shape)\n",
        "\n",
        "\n",
        "# for validation data\n",
        "order = 5\n",
        "ar_feature_val = AR_Coeffs(Val_Data_filtered, order)\n",
        "\n",
        "n_class = 2\n",
        "ar_feature_classes_val = np.zeros((n_class,int(ar_feature_val.shape[0]/n_class),ar_feature_val.shape[1],ar_feature_val.shape[-1]))\n",
        "\n",
        "ar_feature_classes_val[0,:,:,:] = ar_feature_val[class1_trialnum_val,:,:]\n",
        "ar_feature_classes_val[1,:,:,:] = ar_feature_val[class2_trialnum_val,:,:]\n",
        "\n",
        "print(ar_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1zdHj5X0hgJ",
        "outputId": "161d984f-48db-4929-e8a0-6519c199e753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30)\n",
            "(2, 10, 30)\n"
          ]
        }
      ],
      "source": [
        "# calculate Form Factor -  one value for each channel and trial for training data\n",
        "FF_feature = FF_Feature(selected_Data)\n",
        "\n",
        "n_class = 2\n",
        "FF_feature_classes = np.zeros((n_class,int(FF_feature.shape[1]/n_class),FF_feature.shape[0]))\n",
        "\n",
        "FF_feature_classes[0,:,:] = FF_feature.T[class1_trialnum,:]\n",
        "FF_feature_classes[1,:,:] = FF_feature.T[class2_trialnum,:]\n",
        "\n",
        "print(FF_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "FF_feature_val = FF_Feature(Val_Data_filtered)\n",
        "\n",
        "n_class = 2\n",
        "FF_feature_classes_val = np.zeros((n_class,int(FF_feature_val.shape[1]/n_class),FF_feature_val.shape[0]))\n",
        "\n",
        "FF_feature_classes_val[0,:,:] = FF_feature_val.T[class1_trialnum_val,:]\n",
        "FF_feature_classes_val[1,:,:] = FF_feature_val.T[class2_trialnum_val,:]\n",
        "\n",
        "print(FF_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib3MTX3v8xFJ",
        "outputId": "aa5e7133-3509-4ba5-ef87-94d53213c10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 435)\n",
            "(2, 10, 435)\n"
          ]
        }
      ],
      "source": [
        "# calculate cov_Feature -  one covariance matrix for each trial for training data\n",
        "cov_feature = cov_Feature(selected_Data)\n",
        "\n",
        "n_class = 2\n",
        "cov_feature_classes = np.zeros((n_class,int(cov_feature.shape[0]/n_class),cov_feature.shape[1]))\n",
        "\n",
        "cov_feature_classes[0,:,:] = cov_feature[class1_trialnum,:]\n",
        "cov_feature_classes[1,:,:] = cov_feature[class2_trialnum,:]\n",
        "print(cov_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "cov_feature_val = cov_Feature(Val_Data_filtered)\n",
        "\n",
        "n_class = 2\n",
        "cov_feature_classes_val = np.zeros((n_class,int(cov_feature_val.shape[0]/n_class),cov_feature_val.shape[1]))\n",
        "\n",
        "cov_feature_classes_val[0,:,:] = cov_feature_val[class1_trialnum_val,:]\n",
        "cov_feature_classes_val[1,:,:] = cov_feature_val[class2_trialnum_val,:]\n",
        "print(cov_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH3kw21Uivlj",
        "outputId": "e3fe518b-01bb-4a91-a6f0-6f10e8dfd55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30)\n",
            "(2, 10, 30)\n"
          ]
        }
      ],
      "source": [
        "# calculate kurtosis_Feature -  one number for each trial and channel\n",
        "kurt_feature = kurtosis_Feature(selected_Data)\n",
        "\n",
        "\n",
        "n_class = 2\n",
        "kurt_feature_classes = np.zeros((n_class,int(kurt_feature.shape[0]/n_class),kurt_feature.shape[1]))\n",
        "\n",
        "kurt_feature_classes[0,:,:] = kurt_feature[class1_trialnum,:]\n",
        "kurt_feature_classes[1,:,:] = kurt_feature[class2_trialnum,:]\n",
        "\n",
        "print(kurt_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "kurt_feature_val = kurtosis_Feature(Val_Data_filtered)\n",
        "\n",
        "n_class = 2\n",
        "kurt_feature_classes_val = np.zeros((n_class,int(kurt_feature_val.shape[0]/n_class),kurt_feature_val.shape[1]))\n",
        "\n",
        "kurt_feature_classes_val[0,:,:] = kurt_feature_val[class1_trialnum_val,:]\n",
        "kurt_feature_classes_val[1,:,:] = kurt_feature_val[class2_trialnum_val,:]\n",
        "\n",
        "print(kurt_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R45f-I2oC1Yh",
        "outputId": "d15dbaa8-ecc5-4d0d-814f-8bc0746eb9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30)\n",
            "(2, 10, 30)\n"
          ]
        }
      ],
      "source": [
        "# calculate max_freq_Feature -  one max freq for each trail and channel for training data\n",
        "f, max_freq_feature = max_freq_Feature(selected_Data, Fs)\n",
        "\n",
        "\n",
        "n_class = 2\n",
        "max_freq_feature_classes = np.zeros((n_class,int(max_freq_feature.shape[0]/n_class),max_freq_feature.shape[1]))\n",
        "\n",
        "max_freq_feature_classes[0,:,:] = max_freq_feature[class1_trialnum,:]\n",
        "max_freq_feature_classes[1,:,:] = max_freq_feature[class2_trialnum,:]\n",
        "\n",
        "\n",
        "print(max_freq_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "f, max_freq_feature_val = max_freq_Feature(Val_Data_filtered, Fs)\n",
        "\n",
        "\n",
        "n_class = 2\n",
        "max_freq_feature_classes_val = np.zeros((n_class,int(max_freq_feature_val.shape[0]/n_class),max_freq_feature_val.shape[1]))\n",
        "\n",
        "max_freq_feature_classes_val[0,:,:] = max_freq_feature_val[class1_trialnum_val,:]\n",
        "max_freq_feature_classes_val[1,:,:] = max_freq_feature_val[class2_trialnum_val,:]\n",
        "\n",
        "\n",
        "print(max_freq_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ3H-Sj7JG26",
        "outputId": "7b1abe66-4d02-49d8-e924-7483e7b64915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30)\n",
            "(2, 10, 30)\n"
          ]
        }
      ],
      "source": [
        "# calculate mean_freq_Feature -  one mean freq for each trail and channel for training data\n",
        "f, mean_freq_feature = mean_freq_Feature(selected_Data, Fs)\n",
        "\n",
        "n_class = 2\n",
        "mean_freq_feature_classes = np.zeros((n_class,int(max_freq_feature.shape[0]/n_class),max_freq_feature.shape[1]))\n",
        "\n",
        "mean_freq_feature_classes[0,:,:] = mean_freq_feature[class1_trialnum,:]\n",
        "mean_freq_feature_classes[1,:,:] = mean_freq_feature[class2_trialnum,:]\n",
        "\n",
        "print(mean_freq_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "f, mean_freq_feature_val = mean_freq_Feature(Val_Data_filtered, Fs)\n",
        "\n",
        "n_class = 2\n",
        "mean_freq_feature_classes_val = np.zeros((n_class,int(mean_freq_feature_val.shape[0]/n_class),mean_freq_feature_val.shape[1]))\n",
        "\n",
        "mean_freq_feature_classes_val[0,:,:] = mean_freq_feature_val[class1_trialnum_val,:]\n",
        "mean_freq_feature_classes_val[1,:,:] = mean_freq_feature_val[class2_trialnum_val,:]\n",
        "\n",
        "print(mean_freq_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KeWHXA_MHkW",
        "outputId": "f178d705-134d-41dd-a1b4-18e1673124bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: scipy.argmax is deprecated and will be removed in SciPy 2.0.0, use numpy.argmax instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: scipy.argmax is deprecated and will be removed in SciPy 2.0.0, use numpy.argmax instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: scipy.trapz is deprecated and will be removed in SciPy 2.0.0, use numpy.trapz instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 50, 30, 5)\n",
            "(2, 10, 30, 5)\n"
          ]
        }
      ],
      "source": [
        "# selected band energy - one energy for each trail and channel for training data\n",
        "band_energy_feature = band_energy_Feature1(Train_Data, Fs)\n",
        "\n",
        "n_class = 2\n",
        "band_energy_feature_classes = np.zeros((n_class,int(band_energy_feature.shape[0]/n_class),band_energy_feature.shape[1],band_energy_feature.shape[-1]))\n",
        "band_energy_feature_classes[0,:,:,:] = band_energy_feature[class1_trialnum,:,:]\n",
        "band_energy_feature_classes[1,:,:,:] = band_energy_feature[class2_trialnum,:,:]\n",
        "\n",
        "print(band_energy_feature_classes.shape)\n",
        "\n",
        "# for validation data\n",
        "band_energy_feature_val = band_energy_Feature1(Val_data, Fs)\n",
        "\n",
        "n_class = 2\n",
        "band_energy_feature_classes_val = np.zeros((n_class,int(band_energy_feature_val.shape[0]/n_class),band_energy_feature_val.shape[1],band_energy_feature_val.shape[-1]))\n",
        "\n",
        "band_energy_feature_classes_val[0,:,:,:] = band_energy_feature_val[class1_trialnum_val,:,:]\n",
        "band_energy_feature_classes_val[1,:,:,:] = band_energy_feature_val[class2_trialnum_val,:,:]\n",
        "\n",
        "print(band_energy_feature_classes_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(band_energy_feature_classes[0,10,20,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cailYAV8eHxR",
        "outputId": "f0de4ab7-241c-4be7-e2b4-a309f1f0ee31"
      },
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.42834684e-02 3.23457877e+00 1.64231208e+01 1.05305129e+02\n",
            " 5.87435215e+01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "id": "GT3JjqkoKBD3"
      },
      "outputs": [],
      "source": [
        "def fisher_score_cal(bothclass_data,class1_data,class2_data):\n",
        "  # calculate fisher score for each channel\n",
        "  mean_bothclass = np.mean(bothclass_data)\n",
        "  mean_class1 = np.mean(class1_data)\n",
        "  mean_class2 = np.mean(class2_data)\n",
        "\n",
        "  var_class1 = np.var(class1_data)\n",
        "  var_class2 = np.var(class2_data)\n",
        "\n",
        "  score = ((mean_bothclass-mean_class1)**2 + (mean_bothclass-mean_class2)**2)/(var_class1+var_class2)\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {
        "id": "VPh4h9DuPxrJ"
      },
      "outputs": [],
      "source": [
        "fisher_score = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {
        "id": "OB8B15atacnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077f939b-8674-4488-edbc-af4a249fb492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04236799907745556, 1.030797623451539, 0.08894152170369396, 0.05981920092971256, 0.03702245989700413, 0.045415425041764185, 0.0656400728821595, 0.04858063598387115, 0.035243724318514955, 0.032040455433936556, 0.014620530750622108, 0.046161772725405906, 0.014705438441535074, 0.1974519145709862, 0.013309565270020075, 0.08022250661200551, 0.01670876857055354, 0.1023107367367277, 0.08100754050673462, 0.07163751761400163, 0.1195029126879982, 0.15333243973681046, 0.1093009700361702, 0.07914780314210683, 0.027132307881094418, 0.04948877670021916, 0.24833882033649218, 0.1661775805285162, 0.37796364325493054, 0.26184782038235305]\n",
            "(array([ 1, 13, 17, 20, 21, 22, 26, 27, 28, 29]),)\n"
          ]
        }
      ],
      "source": [
        "n_feature_each_channel = 1\n",
        "n_channel = 30\n",
        "selected_feature = band_energy_feature_classes\n",
        "\n",
        "for i in range(n_channel):\n",
        "  selected_channel =  i\n",
        "  index = np.arange((selected_channel)*n_feature_each_channel, (selected_channel+1)*n_feature_each_channel)\n",
        "  fisher_score.append(fisher_score_cal(band_energy_feature_classes[:,:,:,3], band_energy_feature_classes[0,:,index,3], band_energy_feature_classes[1,:,index,3]))\n",
        "\n",
        "print(fisher_score)\n",
        "\n",
        "findeds = np.where(np.asarray(fisher_score) > 0.1)\n",
        "print(findeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YPLkB5v0LYHP"
      },
      "outputs": [],
      "source": [
        "# it seems that non of the features give us high fisher score just by them self"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq3 = [13, 26, 28]\n",
        "freq4 = [ 1, 13, 17, 20, 21, 22, 26, 27, 28, 29]"
      ],
      "metadata": {
        "id": "5j1oPgVWf0fS"
      },
      "execution_count": 457,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfTpR_5bLO7Y",
        "outputId": "927d8f71-695d-4047-887d-6ede531fb0ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 13) (50, 13) (100, 13)\n",
            "(10, 13) (10, 13) (20, 13)\n"
          ]
        }
      ],
      "source": [
        "# for training data\n",
        "# lets check some features together # \n",
        "group_features_class1 = np.concatenate((band_energy_feature_classes[0,:,freq4,3], band_energy_feature_classes[0,:,freq3,2]), axis=0).T\n",
        "group_features_class2 = np.concatenate((band_energy_feature_classes[1,:,freq4,3], band_energy_feature_classes[1,:,freq3,2]), axis=0).T\n",
        "group_features_both = np.concatenate((group_features_class1,group_features_class2), axis=0)\n",
        "\n",
        "\n",
        "# normalize feature matrices \n",
        "# group_features_both = stats.zscore(group_features_both, axis=1)\n",
        "# group_features_class1 = group_features_both[class1_trialnum,:]\n",
        "# group_features_class2 = group_features_both[class2_trialnum,:]\n",
        "\n",
        "\n",
        "print(group_features_class1.shape, group_features_class2.shape, group_features_both.shape)\n",
        "\n",
        "# for validation\n",
        "group_features_class1_val = np.concatenate((band_energy_feature_classes_val[0,:,freq4,3], band_energy_feature_classes_val[0,:,freq3,2]), axis=0).T\n",
        "group_features_class2_val = np.concatenate((band_energy_feature_classes_val[1,:,freq4,3], band_energy_feature_classes_val[1,:,freq3,2]), axis=0).T\n",
        "group_features_both_val = np.concatenate((group_features_class1_val,group_features_class2_val), axis=0)\n",
        "\n",
        "# normalize feature matrices \n",
        "# group_features_both_val = stats.zscore(group_features_both_val, axis=1)\n",
        "# group_features_class1_val = group_features_both_val[class1_trialnum_val,:]\n",
        "# group_features_class2_val = group_features_both_val[class2_trialnum_val,:]\n",
        "\n",
        "print(group_features_class1_val.shape, group_features_class2_val.shape, group_features_both_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txo6fu6MTp9T",
        "outputId": "56214a0a-d1ce-45d3-b0e7-7774cf653abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13, 13) (13, 13)\n",
            "(13, 1) (13, 1) (13, 1)\n",
            "(13, 13)\n",
            "final score is:  0.0023020792718093566\n"
          ]
        }
      ],
      "source": [
        "# score\n",
        "\n",
        "# within class matrices\n",
        "S1 = np.zeros((group_features_class1.shape[1],group_features_class1.shape[1]))\n",
        "S2 = np.zeros((group_features_class2.shape[1],group_features_class2.shape[1]))\n",
        "\n",
        "n_trials = 50\n",
        "n_class = 2\n",
        "for i in range(n_trials):\n",
        "  S1 += (group_features_class1[i,:] - np.mean(group_features_class1[i,:])) @ (group_features_class1[i,:] - np.mean(group_features_class1[i,:])).T\n",
        "  S2 += (group_features_class2[i,:] - np.mean(group_features_class2[i,:])) @ (group_features_class2[i,:] - np.mean(group_features_class2[i,:])).T\n",
        "\n",
        "S1 /= n_trials\n",
        "S2 /= n_trials  \n",
        "print(S1.shape, S2.shape)\n",
        "\n",
        "Sw = S1+S2\n",
        "\n",
        "# between class matrix\n",
        "Sb = np.zeros((group_features_class1.shape[1],group_features_class1.shape[1]))\n",
        "\n",
        "mean_all = np.expand_dims(np.mean(group_features_both, axis=0), axis=1)\n",
        "mean_class1 = np.expand_dims(np.mean(group_features_class1, axis=0), axis=1)\n",
        "mean_class2 = np.expand_dims(np.mean(group_features_class2, axis=0), axis=1)\n",
        "\n",
        "print(mean_all.shape, mean_class1.shape, mean_class2.shape)\n",
        "\n",
        "Sb = ((mean_class1-mean_all) @ (mean_class1-mean_all).T) + ((mean_class2-mean_all) @ (mean_class2-mean_all).T)\n",
        "\n",
        "print(Sb.shape)\n",
        "\n",
        "# final score\n",
        "J = np.trace(Sb)/np.trace(Sw)\n",
        "print('final score is: ', J)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPrFzslae1_m",
        "outputId": "97a7cdaf-f6aa-4d50-f56f-df16e8afbf56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2) (20, 2)\n"
          ]
        }
      ],
      "source": [
        "# change y to one hot labels\n",
        "# we have 2 classes, so our MLP will have 2 output neurons which one neuron \n",
        "# will be one and the others zero\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_tr_hot = to_categorical(Train_Label)\n",
        "y_val_hot = to_categorical(Val_Label)\n",
        "\n",
        "print(y_tr_hot.shape, y_val_hot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the K-fold Cross Validator\n",
        "# Setting up the layers\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Softmax\n",
        "num_folds = 5\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "input_layer_size = group_features_both.shape[1]\n",
        "for train, test in kfold.split(group_features_both, y_tr_hot):\n",
        "  # CREATE MLP MODEL\n",
        "  # Setting up the layers\n",
        "\n",
        "\n",
        "\n",
        "  model = keras.Sequential([\n",
        "                        Input(shape = (input_layer_size,)), # input layer\n",
        "                        Dense(units = 20), # hidden layer one\n",
        "                        Activation(activation = tf.math.tanh),\n",
        "                        Dense(units = 10), # hidden layer two\n",
        "                        Activation(activation = tf.math.tanh),\n",
        "                        Dense(units = 2), # output layer\n",
        "                        Softmax(axis = 1)\n",
        "  ])\n",
        "\n",
        "  # Compling the model\n",
        "\n",
        "  # make our model ready for training\n",
        "  # 1. optimizer 2. loss function 3. metrics\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = keras.optimizers.SGD(learning_rate=0.001),\n",
        "      loss = keras.losses.CategoricalCrossentropy(),\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "\n",
        "  hist = model.fit(\n",
        "      group_features_both[train],\n",
        "      y_tr_hot[train],\n",
        "      batch_size=5,\n",
        "      epochs=100,\n",
        "  )\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(group_features_both[test], y_tr_hot[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Evaluations, test the model on test data using keras\n",
        "  results = model.evaluate(group_features_both_val, y_val_hot)\n",
        "\n",
        "  print(results)\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQIcJeRkAKGz",
        "outputId": "0a09bf8f-5dcb-44d8-f6ba-1413a04b0b8f"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.8098 - accuracy: 0.4750\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7911 - accuracy: 0.4875\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7754 - accuracy: 0.5125\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7647 - accuracy: 0.5625\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7545 - accuracy: 0.5750\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7455 - accuracy: 0.5750\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7374 - accuracy: 0.5750\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7299 - accuracy: 0.5750\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7225 - accuracy: 0.5750\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7170 - accuracy: 0.5750\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7113 - accuracy: 0.5750\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7091 - accuracy: 0.5375\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7039 - accuracy: 0.5375\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7001 - accuracy: 0.5375\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6946 - accuracy: 0.5375\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6904 - accuracy: 0.5250\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6892 - accuracy: 0.5125\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.5375\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.5625\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6779 - accuracy: 0.5500\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6742 - accuracy: 0.5750\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6713 - accuracy: 0.5750\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6703 - accuracy: 0.5875\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6693 - accuracy: 0.5500\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.5875\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6666 - accuracy: 0.5625\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6672 - accuracy: 0.5500\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6643 - accuracy: 0.5875\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6640 - accuracy: 0.5875\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6628 - accuracy: 0.6000\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6624 - accuracy: 0.5875\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6649 - accuracy: 0.5875\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6616 - accuracy: 0.5875\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6623 - accuracy: 0.6125\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6627 - accuracy: 0.5750\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6609 - accuracy: 0.6000\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6582 - accuracy: 0.6125\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.6000\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6568 - accuracy: 0.6250\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6569 - accuracy: 0.6000\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6582 - accuracy: 0.6125\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6562 - accuracy: 0.6250\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6546 - accuracy: 0.6125\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6554 - accuracy: 0.6250\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6536 - accuracy: 0.6125\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6554 - accuracy: 0.6125\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6540 - accuracy: 0.6000\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6536 - accuracy: 0.6125\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6526 - accuracy: 0.6000\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6528 - accuracy: 0.6250\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6522 - accuracy: 0.6250\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6516 - accuracy: 0.6125\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6515 - accuracy: 0.6000\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6533 - accuracy: 0.6125\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6516 - accuracy: 0.6125\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6509 - accuracy: 0.6000\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6509 - accuracy: 0.6250\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.6125\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6514 - accuracy: 0.6000\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6496 - accuracy: 0.6250\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6508 - accuracy: 0.6000\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6504 - accuracy: 0.5875\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.6125\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6497 - accuracy: 0.6000\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6489 - accuracy: 0.6250\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.6125\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6495 - accuracy: 0.6125\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.6000\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6484 - accuracy: 0.6125\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6487 - accuracy: 0.6125\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6471 - accuracy: 0.6250\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.6000\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6472 - accuracy: 0.6125\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6471 - accuracy: 0.6250\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6467 - accuracy: 0.6125\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6461 - accuracy: 0.5875\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6461 - accuracy: 0.6125\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6474 - accuracy: 0.6250\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6473 - accuracy: 0.6125\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6456 - accuracy: 0.6250\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6454 - accuracy: 0.6250\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6453 - accuracy: 0.6125\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6450 - accuracy: 0.6250\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6454 - accuracy: 0.6125\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6450 - accuracy: 0.6125\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6453 - accuracy: 0.6125\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6454 - accuracy: 0.6250\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6437 - accuracy: 0.6250\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6439 - accuracy: 0.6250\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6440 - accuracy: 0.6000\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6441 - accuracy: 0.6125\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6445 - accuracy: 0.6250\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6437 - accuracy: 0.6250\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6430 - accuracy: 0.6250\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6433 - accuracy: 0.6250\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6429 - accuracy: 0.6125\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6440 - accuracy: 0.6125\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.6250\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6435 - accuracy: 0.6000\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6424 - accuracy: 0.6125\n",
            "Score for fold 1: loss of 0.8809650540351868; accuracy of 15.000000596046448%\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7262 - accuracy: 0.5000\n",
            "[0.7261823415756226, 0.5]\n",
            "Training for fold 2 ...\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9484 - accuracy: 0.4750\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7356 - accuracy: 0.5125\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6867 - accuracy: 0.5250\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.5250\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6712 - accuracy: 0.5500\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6626 - accuracy: 0.5625\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6654 - accuracy: 0.5875\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6628 - accuracy: 0.6000\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6638 - accuracy: 0.6250\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6543 - accuracy: 0.6250\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6524 - accuracy: 0.6375\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6712 - accuracy: 0.6375\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6567 - accuracy: 0.6375\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6500 - accuracy: 0.6125\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6463 - accuracy: 0.6250\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6477 - accuracy: 0.6500\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6385 - accuracy: 0.6625\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6335 - accuracy: 0.6875\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6377 - accuracy: 0.6500\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6336 - accuracy: 0.6625\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6305 - accuracy: 0.6875\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7051 - accuracy: 0.6250\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6614 - accuracy: 0.6375\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6543 - accuracy: 0.6250\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6329 - accuracy: 0.6750\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6244 - accuracy: 0.6750\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6242 - accuracy: 0.6500\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6203 - accuracy: 0.6875\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.6750\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6242 - accuracy: 0.6750\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6314 - accuracy: 0.6625\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6205 - accuracy: 0.6875\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.6625\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6217 - accuracy: 0.6875\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.6625\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6244 - accuracy: 0.6625\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6275 - accuracy: 0.6875\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6195 - accuracy: 0.6500\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6215 - accuracy: 0.6625\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.6875\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6275 - accuracy: 0.6625\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6209 - accuracy: 0.6875\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.6750\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6191 - accuracy: 0.6625\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6112 - accuracy: 0.7000\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6170 - accuracy: 0.6875\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6228 - accuracy: 0.6875\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6161 - accuracy: 0.6875\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6242 - accuracy: 0.6875\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6183 - accuracy: 0.6750\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.6875\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6131 - accuracy: 0.6750\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6085 - accuracy: 0.6875\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6300 - accuracy: 0.6500\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6217 - accuracy: 0.6750\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.6750\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6120 - accuracy: 0.6875\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.6500\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6118 - accuracy: 0.6875\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6210 - accuracy: 0.7000\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6178 - accuracy: 0.6625\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6201 - accuracy: 0.6875\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.6625\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6115 - accuracy: 0.6750\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6119 - accuracy: 0.6750\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6111 - accuracy: 0.7000\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6100 - accuracy: 0.7000\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6272 - accuracy: 0.6625\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.6625\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6192 - accuracy: 0.6750\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6136 - accuracy: 0.6750\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7000\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6157 - accuracy: 0.6750\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6175 - accuracy: 0.6500\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6040 - accuracy: 0.6875\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6121 - accuracy: 0.6750\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.6875\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6027 - accuracy: 0.6875\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6064 - accuracy: 0.6875\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6058 - accuracy: 0.6750\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6022 - accuracy: 0.7125\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6139 - accuracy: 0.6625\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6048 - accuracy: 0.7000\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6012 - accuracy: 0.7000\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6023 - accuracy: 0.7000\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5987 - accuracy: 0.6750\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5981 - accuracy: 0.6875\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.6750\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6072 - accuracy: 0.6875\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5955 - accuracy: 0.7000\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6010 - accuracy: 0.6750\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6072 - accuracy: 0.6750\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7000\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5936 - accuracy: 0.7000\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6015 - accuracy: 0.6875\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5953 - accuracy: 0.7250\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.6625\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6055 - accuracy: 0.7000\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.5915 - accuracy: 0.7000\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6078 - accuracy: 0.6625\n",
            "Score for fold 2: loss of 0.7205972075462341; accuracy of 55.000001192092896%\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9630 - accuracy: 0.2000\n",
            "[0.9630073308944702, 0.20000000298023224]\n",
            "Training for fold 3 ...\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7743 - accuracy: 0.4125\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7618 - accuracy: 0.4125\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7539 - accuracy: 0.4250\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7468 - accuracy: 0.4625\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7411 - accuracy: 0.4625\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7361 - accuracy: 0.4625\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7312 - accuracy: 0.4875\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7288 - accuracy: 0.4750\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7230 - accuracy: 0.5000\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7194 - accuracy: 0.4750\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7162 - accuracy: 0.4500\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7147 - accuracy: 0.4875\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7130 - accuracy: 0.5250\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7098 - accuracy: 0.5000\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7083 - accuracy: 0.5000\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7016 - accuracy: 0.5125\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6952 - accuracy: 0.5250\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6902 - accuracy: 0.5125\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6838 - accuracy: 0.5500\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6834 - accuracy: 0.5750\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.6125\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6807 - accuracy: 0.5875\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6839 - accuracy: 0.5750\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6792 - accuracy: 0.5875\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.5875\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6795 - accuracy: 0.5875\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6821 - accuracy: 0.5875\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6788 - accuracy: 0.6125\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6757 - accuracy: 0.6125\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6775 - accuracy: 0.6000\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6747 - accuracy: 0.6125\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6789 - accuracy: 0.5875\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6649 - accuracy: 0.6375\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6764 - accuracy: 0.5875\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6696 - accuracy: 0.6125\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6696 - accuracy: 0.6125\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6668 - accuracy: 0.6125\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6691 - accuracy: 0.6250\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6722 - accuracy: 0.6125\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6705 - accuracy: 0.6000\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6667 - accuracy: 0.6250\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6680 - accuracy: 0.6125\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6708 - accuracy: 0.6000\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6602 - accuracy: 0.6500\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6659 - accuracy: 0.6250\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.6250\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6653 - accuracy: 0.6250\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6650 - accuracy: 0.6375\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6627 - accuracy: 0.6125\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.6375\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6600 - accuracy: 0.6250\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6634 - accuracy: 0.6500\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6636 - accuracy: 0.6375\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6614 - accuracy: 0.6250\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6581 - accuracy: 0.6375\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6607 - accuracy: 0.6125\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6656 - accuracy: 0.6000\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6614 - accuracy: 0.6125\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6630 - accuracy: 0.6125\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6602 - accuracy: 0.6250\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.6000\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6585 - accuracy: 0.6250\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.6125\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6586 - accuracy: 0.6250\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.6125\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6590 - accuracy: 0.6500\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6578 - accuracy: 0.6625\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.6000\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.6250\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.6500\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6561 - accuracy: 0.6375\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6571 - accuracy: 0.6500\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6555 - accuracy: 0.6500\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6561 - accuracy: 0.6625\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6562 - accuracy: 0.6625\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6575 - accuracy: 0.6375\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6598 - accuracy: 0.6625\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6556 - accuracy: 0.6500\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6558 - accuracy: 0.6500\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6549 - accuracy: 0.6625\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6548 - accuracy: 0.6500\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6549 - accuracy: 0.6625\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6551 - accuracy: 0.6625\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.6625\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6529 - accuracy: 0.6750\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6529 - accuracy: 0.6750\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6539 - accuracy: 0.6500\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6532 - accuracy: 0.6750\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.6625\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6556 - accuracy: 0.6500\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.6500\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6543 - accuracy: 0.6375\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6517 - accuracy: 0.6750\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6526 - accuracy: 0.6625\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6517 - accuracy: 0.6625\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6511 - accuracy: 0.6625\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6528 - accuracy: 0.6500\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6515 - accuracy: 0.6625\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6527 - accuracy: 0.6625\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6536 - accuracy: 0.6625\n",
            "Score for fold 3: loss of 0.7717976570129395; accuracy of 40.00000059604645%\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7288 - accuracy: 0.5500\n",
            "[0.7288349866867065, 0.550000011920929]\n",
            "Training for fold 4 ...\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7387 - accuracy: 0.4875\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7213 - accuracy: 0.5500\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7220 - accuracy: 0.5500\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7159 - accuracy: 0.5375\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7106 - accuracy: 0.5500\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7009 - accuracy: 0.5500\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6973 - accuracy: 0.5500\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.5500\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6869 - accuracy: 0.5375\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5500\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6750 - accuracy: 0.5375\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6772 - accuracy: 0.5500\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6694 - accuracy: 0.5625\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.6000\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6667 - accuracy: 0.6000\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6697 - accuracy: 0.5750\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6618 - accuracy: 0.6000\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6664 - accuracy: 0.5875\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6616 - accuracy: 0.5875\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6615 - accuracy: 0.6250\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6598 - accuracy: 0.6125\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6577 - accuracy: 0.6375\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6547 - accuracy: 0.6375\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.6250\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6625\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.6500\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6549 - accuracy: 0.6375\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6543 - accuracy: 0.6625\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6494 - accuracy: 0.6625\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6475 - accuracy: 0.6875\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6476 - accuracy: 0.6375\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6477 - accuracy: 0.6625\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6416 - accuracy: 0.6625\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6462 - accuracy: 0.6875\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6423 - accuracy: 0.6875\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6411 - accuracy: 0.7125\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.6875\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.7375\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6403 - accuracy: 0.6875\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6446 - accuracy: 0.6625\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.6875\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6376 - accuracy: 0.7000\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6360 - accuracy: 0.7125\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6750\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6370 - accuracy: 0.7000\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6363 - accuracy: 0.7000\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6354 - accuracy: 0.6625\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6416 - accuracy: 0.7250\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6427 - accuracy: 0.7125\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6355 - accuracy: 0.6750\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6322 - accuracy: 0.7000\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6313 - accuracy: 0.7250\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.7250\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6261 - accuracy: 0.7125\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6298 - accuracy: 0.7250\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6265 - accuracy: 0.7125\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6289 - accuracy: 0.7250\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6239 - accuracy: 0.7250\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6236 - accuracy: 0.7250\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.7250\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6265 - accuracy: 0.7125\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6306 - accuracy: 0.7125\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6227 - accuracy: 0.7250\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6220 - accuracy: 0.7125\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.7500\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6222 - accuracy: 0.7125\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6209 - accuracy: 0.7125\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.7250\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.7500\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6296 - accuracy: 0.7125\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6292 - accuracy: 0.7250\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6289 - accuracy: 0.6875\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6211 - accuracy: 0.7375\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6154 - accuracy: 0.7125\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6125 - accuracy: 0.7375\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.7000\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6100 - accuracy: 0.7125\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6146 - accuracy: 0.7375\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6098 - accuracy: 0.7250\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6105 - accuracy: 0.7125\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.7250\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6099 - accuracy: 0.7250\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6083 - accuracy: 0.7250\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.7375\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6181 - accuracy: 0.7125\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.7500\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.7250\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6041 - accuracy: 0.7250\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6110 - accuracy: 0.7250\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6038 - accuracy: 0.7375\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6102 - accuracy: 0.7000\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.7250\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6036 - accuracy: 0.7375\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.7375\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.7250\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6019 - accuracy: 0.7250\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.7375\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6111 - accuracy: 0.7000\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.7125\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.7250\n",
            "Score for fold 4: loss of 0.7160295248031616; accuracy of 50.0%\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7276 - accuracy: 0.5500\n",
            "[0.727567195892334, 0.550000011920929]\n",
            "Training for fold 5 ...\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.9304 - accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8879 - accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8416 - accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.8168 - accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7919 - accuracy: 0.4875\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7745 - accuracy: 0.4750\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7608 - accuracy: 0.4750\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7515 - accuracy: 0.4625\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7427 - accuracy: 0.4625\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7349 - accuracy: 0.4500\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.7311 - accuracy: 0.4625\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7255 - accuracy: 0.4500\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7210 - accuracy: 0.4875\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7152 - accuracy: 0.4625\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7163 - accuracy: 0.4125\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7110 - accuracy: 0.4375\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7090 - accuracy: 0.4875\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7114 - accuracy: 0.4375\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7043 - accuracy: 0.4750\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7050 - accuracy: 0.5125\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7059 - accuracy: 0.5000\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.4875\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.4750\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6973 - accuracy: 0.4750\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.5000\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6982 - accuracy: 0.4750\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6951 - accuracy: 0.5000\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6951 - accuracy: 0.4625\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.4875\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.4750\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5000\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6889 - accuracy: 0.4875\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.4750\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.4750\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.4750\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.4625\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.4875\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5000\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5250\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5125\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5250\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.5375\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6825 - accuracy: 0.5375\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5500\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6826 - accuracy: 0.5625\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6830 - accuracy: 0.5500\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.5750\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.5875\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6815 - accuracy: 0.5875\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.5875\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.5750\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6785 - accuracy: 0.6000\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.6000\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6794 - accuracy: 0.6000\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.5875\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.6000\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6786 - accuracy: 0.6000\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.6125\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6752 - accuracy: 0.6125\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.6000\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.6125\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.6125\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6729 - accuracy: 0.6250\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6757 - accuracy: 0.6250\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6753 - accuracy: 0.6125\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.6125\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.6125\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.6500\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6730 - accuracy: 0.6250\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.6250\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6707 - accuracy: 0.6125\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6725 - accuracy: 0.6375\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.6375\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6125\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.6125\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.6125\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.6375\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.6375\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.6125\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.6500\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.6375\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.6375\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6693 - accuracy: 0.6375\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6699 - accuracy: 0.6375\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.6375\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.6375\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6667 - accuracy: 0.6500\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.6375\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6676 - accuracy: 0.6500\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6676 - accuracy: 0.6375\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6672 - accuracy: 0.6500\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.6375\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.6375\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6680 - accuracy: 0.6375\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.6375\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.6375\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6676 - accuracy: 0.6250\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.6375\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6656 - accuracy: 0.6375\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.6664 - accuracy: 0.6250\n",
            "Score for fold 5: loss of 0.7483945488929749; accuracy of 40.00000059604645%\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8097 - accuracy: 0.4000\n",
            "[0.8097482919692993, 0.4000000059604645]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHKzHNENpyu6",
        "outputId": "834f77c3-baa0-4342-9aa4-c831adc4f494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6361 - accuracy: 0.6500\n",
            "[0.6361292600631714, 0.6499999761581421]\n"
          ]
        }
      ],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CI Project",
      "provenance": [],
      "authorship_tag": "ABX9TyP6R6Ww0buhbOv7p1ddq5+2"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}